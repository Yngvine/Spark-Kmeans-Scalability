{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d53f40e",
   "metadata": {},
   "source": [
    "## 1. Configuración del entorno Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73819573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ejecutable: c:\\Users\\2012m\\Desktop\\Spark-Kmeans-Scalability\\.venv\\Scripts\\python.exe\n",
      "Versión Python: 3.11.14 (main, Dec  9 2025, 18:59:10) [MSC v.1944 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Configurar Python ejecutable para Spark (importante en Windows)\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "print(f\"Python ejecutable: {sys.executable}\")\n",
    "print(f\"Versión Python: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6796be3e",
   "metadata": {},
   "source": [
    "## 2. Crear SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff1fc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SparkSession creada exitosamente\n",
      "Versión Spark: 3.5.7\n",
      "Master: local[*]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✓ SparkSession creada exitosamente\")\n",
    "print(f\"Versión Spark: {spark.version}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b2fa24",
   "metadata": {},
   "source": [
    "## 3. Prueba básica con DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e329234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DataFrame creado\n",
      "+-------+----+-------------------+\n",
      "| nombre|edad|          profesion|\n",
      "+-------+----+-------------------+\n",
      "|  Alice|  25|          Ingeniera|\n",
      "|    Bob|  30|Científico de Datos|\n",
      "|Charlie|  35|           Analista|\n",
      "|  Diana|  28|     Desarrolladora|\n",
      "|    Eve|  32|         Arquitecta|\n",
      "+-------+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame simple\n",
    "data = [\n",
    "    (\"Alice\", 25, \"Ingeniera\"),\n",
    "    (\"Bob\", 30, \"Científico de Datos\"),\n",
    "    (\"Charlie\", 35, \"Analista\"),\n",
    "    (\"Diana\", 28, \"Desarrolladora\"),\n",
    "    (\"Eve\", 32, \"Arquitecta\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"nombre\", \"edad\", \"profesion\"])\n",
    "\n",
    "print(\"✓ DataFrame creado\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c9e7a",
   "metadata": {},
   "source": [
    "## 4. Operaciones básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "365f235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros: 5\n",
      "\n",
      "Esquema:\n",
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- edad: long (nullable = true)\n",
      " |-- profesion: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contar registros\n",
    "count = df.count()\n",
    "print(f\"Total de registros: {count}\")\n",
    "\n",
    "# Mostrar esquema\n",
    "print(\"\\nEsquema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3135587b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personas mayores de 30 años:\n",
      "+-------+----+----------+\n",
      "| nombre|edad| profesion|\n",
      "+-------+----+----------+\n",
      "|Charlie|  35|  Analista|\n",
      "|    Eve|  32|Arquitecta|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrar por edad\n",
    "print(\"Personas mayores de 30 años:\")\n",
    "df.filter(df.edad > 30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd920b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estadísticas de edad:\n",
      "+-------+------------------+\n",
      "|summary|              edad|\n",
      "+-------+------------------+\n",
      "|  count|                 5|\n",
      "|   mean|              30.0|\n",
      "| stddev|3.8078865529319543|\n",
      "|    min|                25|\n",
      "|    max|                35|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estadísticas descriptivas\n",
    "print(\"Estadísticas de edad:\")\n",
    "df.describe([\"edad\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2bcd05",
   "metadata": {},
   "source": [
    "## 5. Prueba con datos más grandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81994dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma: 500500\n",
      "Promedio: 500.5\n",
      "Máximo: 1000\n",
      "Mínimo: 1\n",
      "\n",
      "✓ Operaciones RDD completadas exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Crear un RDD con números del 1 al 1000\n",
    "rdd = spark.sparkContext.parallelize(range(1, 1001))\n",
    "\n",
    "# Operaciones RDD\n",
    "suma = rdd.sum()\n",
    "promedio = rdd.mean()\n",
    "maximo = rdd.max()\n",
    "minimo = rdd.min()\n",
    "\n",
    "print(f\"Suma: {suma}\")\n",
    "print(f\"Promedio: {promedio}\")\n",
    "print(f\"Máximo: {maximo}\")\n",
    "print(f\"Mínimo: {minimo}\")\n",
    "print(\"\\n✓ Operaciones RDD completadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcc4bc7",
   "metadata": {},
   "source": [
    "## 6. Prueba con SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cddfe786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edad promedio por profesión:\n",
      "+-------------------+-------------+\n",
      "|          profesion|edad_promedio|\n",
      "+-------------------+-------------+\n",
      "|           Analista|         35.0|\n",
      "|         Arquitecta|         32.0|\n",
      "|Científico de Datos|         30.0|\n",
      "|     Desarrolladora|         28.0|\n",
      "|          Ingeniera|         25.0|\n",
      "+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registrar DataFrame como tabla temporal\n",
    "df.createOrReplaceTempView(\"personas\")\n",
    "\n",
    "# Ejecutar consulta SQL\n",
    "resultado = spark.sql(\"\"\"\n",
    "    SELECT profesion, AVG(edad) as edad_promedio\n",
    "    FROM personas\n",
    "    GROUP BY profesion\n",
    "    ORDER BY edad_promedio DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Edad promedio por profesión:\")\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fdcd48",
   "metadata": {},
   "source": [
    "## 7. Convertir a Pandas (para visualización)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e346e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame convertido a Pandas:\n",
      "    nombre  edad            profesion\n",
      "0    Alice    25            Ingeniera\n",
      "1      Bob    30  Científico de Datos\n",
      "2  Charlie    35             Analista\n",
      "3    Diana    28       Desarrolladora\n",
      "4      Eve    32           Arquitecta\n",
      "\n",
      "✓ Conversión a Pandas exitosa\n"
     ]
    }
   ],
   "source": [
    "# Convertir DataFrame de Spark a Pandas\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "print(\"DataFrame convertido a Pandas:\")\n",
    "print(pandas_df)\n",
    "print(\"\\n✓ Conversión a Pandas exitosa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654d4af",
   "metadata": {},
   "source": [
    "## 8. Información del cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a45ee209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application ID: local-1767347729336\n",
      "Default Parallelism: 24\n",
      "Spark Version: 3.5.7\n",
      "Python Version: 3.11\n"
     ]
    }
   ],
   "source": [
    "# Información del contexto Spark\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"Application ID: {sc.applicationId}\")\n",
    "print(f\"Default Parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"Spark Version: {sc.version}\")\n",
    "print(f\"Python Version: {sc.pythonVer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b6515",
   "metadata": {},
   "source": [
    "## 9. Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38e74761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "✓ TODAS LAS PRUEBAS COMPLETADAS EXITOSAMENTE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Detener Spark session (opcional - solo si quieres liberar recursos)\n",
    "# spark.stop()\n",
    "# print(\"✓ SparkSession detenida\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✓ TODAS LAS PRUEBAS COMPLETADAS EXITOSAMENTE\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-Kmeans-Scalability",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
